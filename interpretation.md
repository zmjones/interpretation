% Interpretable Statistical Learning Methods
% Zachary M. Jones[^contact]

[^contact]: Email: [zmj@zmjones.com](mailto:zmj@zmjones.com).

\begin{abstract}

Statistical learning methods, which are a flexible class of methods capable of estimating the parameters of complex data generating functions, offer an attractive alternative to conventional methods, which often rely on strong assumptions about the assumed functional form. Statistical learning methods make fewer assumptions about functional form and can estimate a much larger class of data generating functions. A key impediment to the use of statistical learning methods is that they are usually thought of as "black box," in that they cannot be directly interpreted in general. I show that this need not be the case by explaining and implementing methods that are capable of making any method that generates predictions interpretable. This allows researchers to learn about relationships in the data without having to prespecify their functional form. I illustrate this approach using an application to predictive policing in Chicago, wherein these interpretation methods can be used to show how using historical crime data to predict future crime reinforces socio-demographic divides.
\end{abstract}

## Introduction
 
The social world is often complex, messy, and difficult to describe with a simple set of rules or laws. In contrast, conventional methods *presume* data are generated by relatively simple processes [@king1989unifying; @berk2008statistical]. This hampers our ability to discover relationships in our data that we did not expect and reduces the predictive validity of our models [@miller2009complex; @beck2000improving; @beck1998beyond; @schrodt2014seven; @fariss_jones_2017]. With conventional methods the data cannot tell us directly about relationships which were not prespecified in the model. Furthermore, the manner in which the data can inform our understanding of relationships which *were* prespecified in the model is constrained by the often strong assumptions made about their functional form, i.e, it is difficult to learn about nonlinear and interactive relationships from a linear additive model. One of the benefits of making strong functional form assumptions is that models can be made to be directly interpretable, by, e.g., assuming linearity, simple forms of nonlinearity (e.g., polynomial terms), and/or the absence of interactions involving more than, say, two variables.[^interpretability]. This comes at a substantial price, however. Conventional methods can miss interesting structure in data when that structure cannot be captured by the assumed model, which means that this information is not extracted from the data, damaging our ability to improve our understanding and degrading the quality of our predictions.

[^interpretability]: Interpretability can be thought of in a number of ways (See, e.g., ...), but here I use to to mean models which have additive components which can be visualized, e.g., for continuous inputs, functions of two or fewer covariates.

Advances in computing have allowed the development of statistical learning methods which allow us to make fewer assumptions about the structure in our data and consequentially to discover unexpected relationships in our data [@friedman2001elements]. Statistical learning methods differ from conventional statistical methods in that the number of parameters estimated is a function of the data, rather than specified by the researcher. This gives statistical learning methods the ability to find relationships in data that were not prespecified. This ability to find the unexpected, however, frequently makes the outputs from these methods uninterpretable, which, I argue, has prevented their wide use in the social sciences, where the focus is on substantive insights in addition to prediction [@fariss2015enhancing; @ward2010perils; @hill2014empirical]. This is not generally a problem with conventional statistical methods because we *assume* that the function that generated the data has a relatively simple functional form which is easily interpretable.

I describe and implement general tools which can make any statistical learning method interpretable in ways that allow social scientists to answer questions about the importance of covariates, the nature of their estimated relationship with an outcome, the presence of interactions amongst said covariates (and their shape), and also explanations for predictions for specific observations or instances [@friedman2001greedy; @friedman2008predictive; @hooker2012generalized; @hooker2004discovering; @goldstein2015peeking]. I have made these methods available in an accessible manner via four `R` packages: the **M**achine **L**earning in **R** (`mlr`) package, which provides infrastructure for fitting, tuning, and evaluating a wide variety statistical learning methods, the **E**xploratory **D**ata **A**nalysis using **R**andom **F**orests (`edarf`) package, which contains interpretation methods specific to random forests, a particularly attractive statistical learning method for social scientists, **M**onte-**C**arlo **M**ethods for **P**rediction **F**unctions (`mmpf`) which allows prediction functions to be marginalized to depend on a subset of the covariates, and is itself is used in both of the aforementioned packages, and finally **F**unctional **A**nalysis of **V**ariance (`fanova`) which projects an estimated regression model onto a set of lower dimensional functions which are interpretable [@edarf; @mlr; @mmpf; @hooker2012generalized].

<!-- For example consider a linear model of the form $$y_i = \alpha + \beta_1 x_i + \beta_2 z_i + \beta_3 x_i z_i + \epsilon, \; i = {1, \ldots, n}.$$ Assuming that a moment (e.g., the expectation) of the conditional distribution of $y_i | x_i, z_i$ can be approximated using only ${\alpha, \beta_1, \beta_2, \beta_3}$ allows us to summarize the relationships between ${\mathbf{x}, \mathbf{z}}$ and $\mathbf{y}$ using only $\beta_1$, $\beta_2$ and $\beta_3$. These are assertions about the relationship between $\{\mathbf{x}, \mathbf{z}\}$ and $\mathbf{y}$ and depending on the manner in which our assertions are wrong, this summary can be misleading. Say that the true data generating process was $$y_i = f_0 + f_x(x_i) + f_z(z_i) + f_{xz}(x_i, z_i) + \epsilon.$$ Here the $f_{.}$ are arbitrary functions of their inputs which are *not* linear. Then the conventional model, though it correctly specified that there  -->
<!-- , as in a linear and additive model, where each of the parameters represents the additive effect of each covariate. Statistical learning methods, since they do not presume that the data were generated by such a process, are also not necessarily directly interpretable. -->
<!-- Modeling social data using conventional statistical methods often involves an informal specification [@fariss2014reproduction; @gelman2014statistical]. In practice, predictively validity is often given short shrift, with emphasis instead placed on the substantive interpretation of the model [@ward2010perils]. This in turn means that empirical models may convey less information than is contained in the data; that is, a potentially more complex model is necessary to model generalizable structure in the data [@fariss2014reproduction]. -->

The reason that statistical learning methods do not generally give interpretable outputs is that the manner in which they estimate a model, which is necessitated by their ability to learn complex relationships which were not presupposed, results in an output that, even if the true model *is* human interpretable, would hide that fact: a "black box". Throughout this paper I will use a simple simulated example

$$f(w, x, y, z) = x + y^2 + xw + \sin(z)$$ {#eq:dgp}
$$y = f + \epsilon$$

where $w$, $y$, and $z$ are uniformly distributed between $-2$ and $2$, $x$ is equal to $-1, 0, 1$ with equal probability, and $\epsilon$ is drawn from a standard normal distribution. 

The way a regression tree estimates a model (i.e., how it arrives at predictions), is by subsetting (i.e., partitioning or grouping) $y$ using the covariates repeatedly so that the variability of $y$ within these subsets is minimized. This amounts to finding values of $y$ which are similar using the covariates, similar to what ordinary least squares aims to do, but with substantially weaker assumptions about how the covariates are related to $y$. The result of this process is a nested set of subsets, a hierarchical structure (a tree), which describes the series of subsets that were created: rules which describe the final subset an observation ends up in. As can be seen in Figure \ref{fig:tree} this output is substantially more difficult to interpret than ordinary least squares. This problem is exacerbated when the estimated tree is larger (more complex), or when it is combined with other trees as in ensemble methods like boosted trees or random forests, which are generally preferable for reasons described in later sections. While this data generating function *could* be estimated with conventional methods, it would require the specification of a dummy variable for each level of $x$ interacted with $w$, a third degree polynomial expansion of $z$ to approximate the sine function between $-2$ and $2$, and a squared term for $y$.[^model]:

[^model]: Specifically, a conventional linear model could estimate this model with the following terms $\mathbb{I}(x = -1)$, $\mathbb{I}(x = 0)$, $\mathbb{I}(x == 1)$, $\mathbb{I}(x = -1)w$, $\mathbb{I}(x = 0)w$, $\mathbb{I}(x == 1)w$, $y^2$, $z$, $z^2$, $z^3$.

Statistical learning methods can estimate all of these terms without prespecification, and the interpretation methods I will describe herein can extract these components.

![A model estimated via a regression tree from 5000 samples from the data generating function in Equation \ref{eq:dgp}. Each circle indicates a subset or partition defined on the labeled covariate, with the rule for creating the subset listed on the line below the circle. The final subsets, or terminal nodes, shown in grey, give them predictions of the tree, the number of observations that fall into that subset, and the mean square error of the prediction on those observations. This tree was constrained to be simple for this visualization. \label{fig:tree}](tree.png)

<!-- It is possible to summarise the shape of the relationship between the covariates by computing the partial dependence of the covariates on the model's predictions. Partial dependence uses Monte-Carlo methods to compute the marginal relationship between covariates and the model's predictions in a manner similar to the common practice of fixing the values of some covariates and setting the others to arbitrary values [@tomz2003clarify]. Additionally, any model can be fully decomposed as a sum of simpler functions, which can be used to test for additivity, and find interactions [@hooker2004discovering; @hooker2012generalized]. This allows us to maximize our understanding of the relationships encoded in the fitted model. -->

<!-- The importance of covariates to the prediction made by a model is often of central interest in the social sciences, that is, much of the interpretation of statistical models revolves around the size of coefficients which defines the importance of covariates in terms of their effect on the predictions made by the model. While with many conventional statistical methods the parameters of said models directly give the importance of the covariates in this sense, with statistical learning methods a general analogue does not exist. I suggest several more general alternatives to summarise the importance of covariates.  -->

<!-- The flexibility to learn these sorts of relationships in the data is why statistical learning methods have generally low predictive bias, they are designed to adapt to the data. Over-adaptation, i.e., overfitting, can occur with both conventional and statistical learning methods, and can be avoided  -->

<!-- It is often the case with conventional statistical methods that the shape of the relationship between each of the covariates and the outcome (or some transformation thereof) is contrained to be linear. In these cases the shape of the relationship between said covariate and the predictions is easily understood via the estimated coefficients. However, when interactions and nonlinearities are included by, for example, including products or polynomial expansions of covariates, simply looking at the coefficients often doesn't adequately summarise the shape of the relationship between the features and the model's predictions. This is also the case with statistical learning methods, which are generally capable of learning nonlinear and interactive relationships without the need for the researcher to explicitly include product terms or polynomial expansions. However, it is possible to summarise the shape of the relationship between the covariates by computing the partial dependence of the covariates on the model's predictions. Partial dependence uses Monte-Carlo methods to compute the marginal relationship between covariates and the model's predictions in a similar manner to the common practice of fixing the values of some covariates and setting the others to arbitrary values [@tomz2003clarify]. Additionally, any model can be fully decomposed as a sum of simpler functions, which can be used to test for additivity, and find interactions [@hooker2004discovering; @hooker2012generalized]. -->

<!-- A substantial issue with model-based inference is error due to extrapolation. Extrapolation error occurs when a model's behavior is evaluated in regions of the joint distribution which generated the data which have low (possibly zero) density/mass. Points of in regions of low/zero probability (points of extrapolation) can be generated by particular diagnostic methods such as partial dependence and permutation importance, which assume the covariates were drawn from a product distribution, or by researchers directly, as described in @king2006dangers. This is especially easy to do in high dimensions where the covariates are dependent and/or do not have an ellipsoidal joint distribution (e.g., the joint distribution may have holes within its support). Both partial dependence and permutation importance generate summaries of model behavior which depend on points which may have low/zero probability because they both are computed under the assumption that the joint distribution can be factorized into a product of two densities, one over a subset of the covariates which are of interest and one over the remaining covariates. When the joint distribution is not factorizable in this manner the resulting summaries may depend heavily on points drawn from this inappropriately factorized empirical joint distribution. -->

<!-- It is therefore desirable to compute the anamolousness of points at which the model behavior is interepreted with respect to the empirical joint distribution of the data. In the case of a single prediction made from a model (e.g., as in @king2006dangers), said prediction can be accompanied by this measure of anomolousness. In the case of partial dependence and permutation importance, a measure of outlyingness can be used to compute an estimate of model behavior which is inversely weighted by this anomolousness metric, resulting in model summaries which depend less on points which are unlikely under the empirical joint distribution. In this latter case, the performance of various methods that can be used to estimate anomalousness will be evaluated versus the joint distribution in a simulation. -->

<!-- @hooker2004diagnosing formulated the problem of detecting whether points came from a region of low/zero probability as a classification problem wherein a point is classified according to whether it is likely to have come from a multivariate uniform distribution defined by the minimum and maximum of each covariate. An alternative approach is to use data depth, an approach which ranks points according to their distance from a most central point, under a particular distance metric [@liu1999multivariate]. @king2006dangers treats extrapolation detection as a binary classification problem and the attendant classification boundary is defined by the convex hull containing the sample data. An additional approach is to directly estimate the joint density of the data, by, e.g., using a density forest [@criminisi2011decision]. Neither the approach of @hooker2004diagnosing nor that of @criminisi2011decision have a reliable implementation (I will fix that). Additionally, the method of @hooker2004diagnosing can be extended in ways that will make it both a better estimator (in particular, lower variance), and more appropriate to covariate distributions of differing scale [@hothorn2006unbiased]. -->

<!-- In addition to appropriately weighting permutation importance and partial dependence, as well as accompanying point estimates of model behavior with an estimate of the anamalousness of said point, it would be desirable to construct bounds on the pointwise expected behavior of a model given a level of anomalousness, essentially a confidence interval for a prediction based that grows in width as the probability of its supporting points decreases. I am not sure this is possible though. I know of some variance estimators for ensemble methods, but they mimic the extrapolation behavior of the learner [e.g., @wager2014confidence; @sexton2009standard, which produce pointwise variance estimates]. A variance estimator which grew as density/mass of the joint distribution decreased would be desirable for these diagnostic methods as well as things like Bayesian optimization (I have done some work on using ensemble variance estimates to do Bayes optimization with other people in the MLR team). -->

While there have been some applications of statistical learning in the social sciences, their use has been largely limited to instances in which forecasting is the primary goal, with a few notable exceptions [@beck2000improving; @hainmueller2013kernel; @hill2014empirical]. While the feed-forward artificial neural networks and kernel regularized least squares used in @beck2000improving and @hainmueller2013kernel are powerful methods which could be appropriately applied to many problems they are not appropriate for *every* application, and thus it is desirable to be able to use, and interpret the results of the best statistical learning method available for your problem. <!-- Insert introduction to crime prediction here --> The methods described herein do precisely that. Before proceeding to the details of these methods for making statistical learning methods interpretable, I will review why statistical learning methods can outperform conventional methods in common social science tasks and how they can be viewed in the same statistical framework as conventional methods.

<!-- , however, where it is used the goal is often used primarily for forecasting, rather than explanation.  with accurate forecasting being a way to validate said understanding [@beck2000improving; @berk2009forecasting; @berk2006introduction; @berk2008statistical;]. In particular there have been few papers which utilize the aforementioned methods for interpretation, and those that do utilize these methods, or similar ones, do so in a limited fashion [@@hill2014empirical]. Herein I use statistical learning methods to predict the incidence of burglary in Chicago using a large database of reported crimes from the Chicago Police Department which allows the estimation of the dynamic, seasonal, and geographic properties of burglary as reported in the data. -->

<!-- , to-date there have been there are no examples as-yet which utilize both statistical learning methods and the aforementioned interpretation methods. To demonstrate the applicability of statistical learning methods, paired with the aforementioned methods for interpretation, I estimate the relationship between a variety of measures of political violence (civil conflict, civil war, international conflict, violent protests, repression, civilian killings, and terrorism) and structural factors believed to be related to political violence, particularly regime type, which have been repeatedly used as explanatory factors. To do this I utilize a multivariate random forest-like algorithm which has the advantage of making few restrictions on the functional form of the multivariate relationship between violence and the aforementioned structural factors. Decomposing the function learned from the data allows insight into the long-standing debate about the relationship between regime type and violence, which is usually studied with a single type of violence and strict functional form assumptions. I show that the relationship between regime type and violence varies across some types of violence and over time. -->

## Statistical Learning

Statistical learning methods like artificial neural networks, decision trees, random forests, boosting, and others have seen use in the social sciences, but are far from common [@beck2000improving; @grimmer2013text; @hill2014empirical; @montgomery2015informed; @green2012modeling; @montgomery2015tree; @hainmueller2013kernel]. Statistical learning methods can be represented in the same framework as more conventional statistical methods. Here I review these equivalences and highlight both the difficulties and advantages of using statistical learning methods as compared to conventional methods.

Assume that the $(\mathbf{X}, Y)$ are random variables drawn from some probability distribution $\mathbb{P}$ over the sample space $\mathcal{X} \times \mathcal{Y}$, which represents all combinations of $X$ and $Y$ that could be obtained, wherein $Y$ represents an outcome and $\mathbf{X}$ covariates.

We typically estimate the function which maps $\mathbf{X}$ to $Y$ by using sample data presumed drawn from $\mathbb{P}$. Often we are interested in the conditional expectation of $Y| \mathbf{X}$, and we assume that this function depends on some parameters $\mathbf{\theta}$.

$$\mathbb{E} \left[Y | \mathbf{X} = \mathbf{x} \right] = f(\mathbf{x}, \mathbf{\theta})$$

Samples of $Y$ are denoted $\mathbf{y}$ and of $\mathbf{X}$, $\mathbf{x}$. To estimate $f$ we need to define a set of functions which can be searched for the best approximation to $f$, e.g., for a linear regression model the space of all possible values of the $\beta$s. We also need a way to decide which values of these parameters are "best." Finally, we need a way to efficiently search through the set of possible models.

### Defining the Space of Possible Models

A *hypothesis space*, denoted $\mathcal{H}$, is a set which defines all possible functions that could be used to approximate the true data generating function $f$. In the case of a linear regression the hypothesis space is the set of functions which are linear and additive in the parameters $\mathbf{\theta}$. Specifically, for a two parameter simple linear regression of the form $y_i = \theta_0 + \theta_1 x_i + \epsilon$, the hypothesis space is $\mathbb{R}^2$, because $\theta_0$ and $\theta_1$ could each be any number from negative to positive infinity. We can learn a nonlinear relationship between $\mathbf{x}$ and $\mathbf{y}$ if we expand the basis (i.e., the set of functions which describe $\mathbf{x}$) for $\mathbf{x}$ which we can denote $\phi(\mathbf{x})$, by, for example, including polynomials of $\mathbf{x}$, but this must be specified in advance rather than being automatically learned from the data. Here the dimension of $\mathbf{\theta}$ is fixed.

$$h(\mathbf{x}, \mathbf{\theta}) = \mathbf{\theta}^T \mathbf{x}$$ {#eq:lm}

Here $h \in \mathcal{H}$, that is, [eq:lm] is one of the possible models contained in the hypothesis space $\mathcal{H}$. Estimating, or learning, a model from data is searching over the hypothesis space to find the "best" candidate model.

In contrast to linear regression (and conventional methods more broadly), boosting, decision trees, random forests, and some types of neural networks, utilize a data-adaptive number of parameters $\mathbf{\theta}$, making the dimension of the hypothesis space larger. For example a regression tree, as in Figure \ref{fig:tree} can be written as

$$h(\mathbf{x}) = \sum_{m=1}^M w_m \mathbb{I}(\mathbf{x} \in R_m)$$ {#eq:tree}

where $\mathbb{I}(\cdot)$ is the indicator function which equals 1 if $\mathbf{x} \in R_m$ and 0 if not, $R_m$ is the $m$'th  of the final, disjoint, and exhaustive partitions of the data, and $w_m$ is the mean value of $\mathbf{y}$ in this region. Each region $R_m$ is defined by which variables, and which values in those variables, were used to create the final set of partitions in the tree: so their number is data-dependent, generally growing with the data. Again, this increases the size of $\mathcal{H}$.

<!-- If $f$, the true data generating function, is inside $\mathcal{H}$, then it is possible to learn $f$. With conventional methods it is often implausible to suppose that this is the case. Even though, with the smaller hypothesis space used by conventional methods, it is relatively find the $ -->

<!-- In general, a smaller $\mathcal{H}$ means that it is easier to find the $h \in \mathcal{H}$ closest to $f$, but if $f$ is far from *any* $h \in \mathcal{H}$ the approximation error in using $h$ to represent $f$ will be large. Presuming a simple $\mathcal{H}$ when we believe that $f$ is complex induces unecessary approximation error by increasing the distance between the best member of $\mathcal{H}$ and $f$. This is a key problem with using conventional methods in cases where we do not really know how complex $f$ is. Figure \ref{fig:error} shows this graphically. -->

<!-- <insert approximation/model error graph here> -->

### Evaluating Candidate Models

How can we evaluate different candidate models in the hypothesis space? A loss function $\mathcal{L}$ tells us how good a particular $h$ is at predicting $\mathbf{y}$. Specifically, the loss $\mathcal{L}$ is a function which maps predictions made by an approximation to $f$, a member of $\mathcal{H}$, which we've called $h$ but is conventionally referred to as $\hat{f}$, and observed realizations of $Y$, $\mathbf{y}$, to a positive real number. Herein lies a problem though, as we are trying to approximate $f$, not perfectly reconstruct $\mathbf{y}$. $Y$ is assumed herein to be random, and so contains a random and a systematic component, i.e., $\mathbf{y} = f(\mathbf{x}) + \epsilon$. If we could compute the discrepancy between $\hat{f}$ and $f$ rather than $\hat{f}$ and $\mathbf{y}$ we would be able to distinguish between candidate hypotheses $h$ which have overadapated to the sample data and those that have more accurately estimated $f$ and hence will generalize better to new data. However, since we do not know $f$ we have to substitute in $\mathbf{y}$, which conflates systematic error, when the function learned from $(\mathbf{y}, \mathbf{x})$ imperfectly represents $f$, and error due to the irreducible randomness in $Y$. Thus minimizing the loss over the observed data may result in models $\hat{f}$ which are close approximations of $\mathbf{y}$ rather than $f$: over-fitting. This is the opposite of the problem of picking a hypothesis space whose closest member is far from $f$: under-fitting. Both, however, leave us with models which can be rather useless in both learning about the parameters of $f$, that is, explanations about how covariates are related to some outcome, as well as predicting $Y$, which is sometimes the goal in and of itself, and other times is simply a minimal check on the validity of the model. 

Like in the case of under-fitting, there are ways to avoid over-fitting such as minimizing the expected loss over $\mathbb{P}$. The expected loss is usually called the risk, and minimizing it can eliminate optimisim that comes from over-fitting the sample data. However, since $\mathbb{P}$ is unknown this is not directly possible. The sample-average loss is an estimator of the risk, however, it is biased downwards, that is, it is optimistic, when applied to the same data on which the model was estimated. The most common solution to this issue is to estimate the risk by using Monte-Carlo methods like cross-validation or the bootstrap which randomly shuffle the data to make it more difficult for over-fit models to appear better than they are at generalizing beyond the sample data.[^structural] Penalized regression models like the least absolute shrinkage and selection operator (LASSO) or ridge regression make use of this idea to shrink the regression coefficients of a conventional model (sometimes to 0), making it simpler, to avoid overfitting.[^smoothing] Statistical learning methods use this idea to make the model as simple or complex as is necessary to minimize risk.

[^structural]: It is also sometimes possible to derive an upper bound on the risk given the size of $\mathcal{H}$, the amount of data available, and the average loss on the sample data [@vapnik1994measuring; @vapnik1998statistical].
[^smoothing]: Another way of saying this is that regularization or shrinkage makes the model less sensitive to the sample data or that it makes the model smoother.

Choosing a loss function is an application specific task that depends on the costs of making different sorts of prediction errors. With a binary outcome, it might be the case that positive cases are more costly when they are not predicted than are negative cases that are not predicted, or, with a continuous outcome, overprediction might be more costly than underprediction [@berk2009forecasting; @muchlinski2016comparing]. Similarly, how the difference between the observed outcome and the prediction affects the loss also depends on how costly bigger or smaller mistakes are. For example mispredicting cases where persons convicted of violent crimes reoffend after parole is probably more costly than predicting recitivism amongst those convicted of non-violent crimes, and it is possible that in some cases over-predicting recitivism is less consequential than under-predicting it. While squared-loss or misclassification-loss is often used because it makes the third step, the search or optimization strategy, easier, it is not necessarily the most appropriate choice, and what loss function is used has an enormous impact on what function from the hypothesis space is chosen as the best approximation to $f$ since it is *the* criteria for that choice.

<!-- work in the crime forecasting example here -->

### Searching for the Best Model

In most cases, even for a simple linear regression, the hypothesis space is too big to evaluate every possible model it contains. So a way to search through the hypothesis space must be devised. In some cases this can be done with relative ease, but for many hypothesis spaces even the best available optimization methods cannot guarantee that they will find the approximation to the true function topology of loss function over the hypothesis space. However, if the best candidate in the hypothesis space for a linear, additive model is far from the truth: $f$, this guarantee means little, since it may be the case that a candidate taken from a larger hypothesis space, even if it is not known to be the best in said space, is much closer to $f$ than the aforementioned optimal choice in the linear, additive hypothesis space. This is why statistical learning methods are so commonly used for pure prediction tasks.

<!-- insert model graph here -->

For illustrative purposes consider a linear additive model as in [@eq:lm] with a squared loss function. Then the function we want to minimize is the sum of squared errors. In this simple case where we have a convex loss function like squared loss gradient descent suffices to find the parameters which minimize the loss function. 

$$h(\mathbf{\theta}, \mathbf{x}) = \sum_j \theta_j x_j$$
$$\mathcal{L}(\mathbf{y}, h) = \sum_{i = 1}^N \frac{1}{2}(y_i - h)^2$$
$$\nabla \mathcal{L} = \frac{\partial \mathcal{L}}{\partial \mathbf{\theta}} = \frac{1}{N} \sum_{i = 1}^N - x_i(y_i - h)$$

Descending the gradient of the loss $\nabla \mathcal{L}$ given a starting value for $\mathbf{\theta}$ (say $\mathbf{0}$),  converges to the global minimum. From the starting value $\mathbf{\theta}^0$ the sequential update for the next step is

$$\mathbf{\theta}^{(k + 1)} = \mathbf{\theta}^{(k)}  - \gamma_k \nabla \mathcal{L}(\mathbf{\theta}^{(k)})$$

where $\gamma$ controls the size of the steps along the gradient, which can itself be estimated by gradient descent. The procedure stops when $\nabla \mathcal{L}$ is sufficiently small, that is, when the loss function is not decreasing any longer.

In contrast consider boosting, a statistical learning method. Like with the aforementioned linear additive model we want to minimize the loss $\mathcal{L}(\mathbf{y}, h) = \frac{1}{2} \sum_{i = 1}^N (y_i - h)^2$, however now each candidate $h$ has more parameters. In this case it is not possible to $h$ will be estimated sequentially, that is, it is composed of a sequence of models which are combined in the last step. We denote the first $m$ components of $h$ as $h_m$.

$$h_m(\mathbf{\gamma}, \mathbf{\theta}, \mathbf{x}) = \sum_{m = 1}^M \gamma_m \phi_m(\mathbf{x}, \mathbf{\theta}_m)$$ {#eq:boost}

Here $\phi_m$ is a model itself, such as a regression tree like is shown in Figure \ref{fig:tree}. For each $m$ the gradient of $\mathcal{L}$ with respect to $h_m$ is

$$\nabla \mathcal{L}(h_m) = \frac{\partial \mathcal{L}}{\partial h_m} = \sum_{i = 1}^N (y_i - h_{m})$$

and so, like in the case of gradient descent for linear regression

$$h_{m + 1} = h_m - \gamma_m \nabla \mathcal{L}(h_m)$$.

The scaling parameters $\mathbf{\gamma}$ are estimated by a separate application of gradient descent.[^gamma]

[^gamma]: Specifically, $\gamma_m = \arg\min_{\gamma} \mathcal{L}(\mathbf{y}, h_{m - 1} + \gamma h_m)$.

So at each step $m$ boosting using this loss function amounts to fitting $\phi_m(\mathbf{x}, \mathbf{\theta}_m)$ to $\mathbf{y} - h_{m - 1}$: repeatedly refitting the residuals. This amounts to giving higher weight to observations where the sequence of models thus far has performed poorly, and by combining these models as in @eq:boost nonlinear relationships between $\mathbf{x}$ and $\mathbf{y}$ can be estimated without specifying $\phi$s which can estimate nonlinearity. It should also be clear that the resulting output $h_M$, which is a weighted sum of models $\phi_m$, which are themselves estimated on residuals which depend on $h_{m-1}$ is not directly interpretable, even if the data generating function it is approximating is. Again, this property is a necessary consequence of statistical learning methods' capacity for automatically finding patterns (e.g., nonlinearity and/or interaction amongst the covariates) that were not prespecified.

## Predicting Burglary from Crime Data in Chicago

police departments have limited resources and so want to expend them in an optimal fashion

there are a number of companies which make predictions about the incidence of crime

conventional methods are ill-suited to this prediction task

statistical learning methods cannot be "inspected"

there are good reasons to want to inspect models in this instance

since there are sociodemographic biases in the data it is possible that the any model learned from these data can effectively learn about these biases, even if sociodemographic covariates are not included

local inspection might be desirable: why does the forecasting model predict crime incidence or absence in particular times and places

global inspection might be desirable: how does crime vary seasonally (month to month, over the course of a day), geographically, or dynamically (i.e., is burglary "streaky?")

random forests are an easy to use statistical learning method because they can be scaled to "big" data (how big are these data), they can accomodate covariates of all different types, and they don't have very many tuning parameters.

some discussion of forecast errors, and/or a comparison?

interpretation of the output of the model will accompany explanations of the different methods for doing so.

## Interpreting Models

I argue that there are four main types of interpretation which are commonly sought. Functional form describes the shape of the relationship between covariates and the model. Magnitude describes how important covariates are in determining the model's predictions. Interaction detection relates to the estimation of the functional form and magnitude of the relationship between groups of covariates and the model. Reliability relates to how different sources of uncertainty could possibly change the estimated effect.

### Functional Form

Interpreting the functional form of the relationship between the covariates and the model is one of the most common tasks. For a linear and additive model this is described by a single number which is obtained by taking the partial derivative of the model with respect to said covariate; e.g., $\frac{\partial \hat{f}}{\partial \mathbf{x_j}} \sum_j \beta_j \mathbf{x}_j = \beta_j$. The situation is complicated for models like logistic regression where the formulation of the model forces this derivative to depend on $\mathbf{x}_{-j}$, but the principle remains the same. For models which are nonlinear in $\mathbf{x}_j$ this derivative is also not constant, but does not necessarily depend on $\mathbf{x}_{-j}$. For example if $\hat{f} = \beta_1 \mathbf{x}_1 + \beta_2 \mathbf{x}_1^2 + \beta_3 \mathbf{x}_2$ then $\frac{\partial \hat{f}}{\partial \mathbf{x}_1} = \beta_1 + \beta_2 \mathbf{x}_1$ and to interpret the relationship between $\mathbf{x}_1$ and $\hat{f}$ it is necessary to plot $\mathbf{x}_1$ and this derivative at a grid of values on $\mathbf{x}_1$. 

### Magnitude

Magnitude describes how important covariates are in determining a models' predictions. In the case of a linear model this is simply the absolute size of the associated regression coefficients. However, for a nonlinear model this isn't so simple. If the derivative of the model with respect to a covariate is not constant in that model then there is no longer a single number summary of that covariate's influence on the model's predictions. Yet, this might be what is desired, as in @HillJones where the goal was to sift through a large set of potentially relevant covariates in order to focus on ones that were identified as useful predictors by the model.

### Interaction Detection

The identification and description of interactions is a common task as well [@brambor2006understanding]. In the case of a linear model, again, this is fairly straightforward, however, the product terms commonly used to estimate interactions in these models are commonly misused in testing nonlinear hypotheses [@hainmueller2016much; @pepinsky2017visual]. Using statistical learning methods avoids this problem by automatically detecting interactions whilst making few assumptions about their possible functional form. This allows researchers to ask these common questions, like, whether or not the effect of one variable changes as a function of another in a general (i.e., not necessarily monotonically) fashion.

### Reliability

Most of the above, when computed using conventional methods, are or can be accompanied by estimates of sampling variability. In some cases this can be a useful measure of reliability but in many cases the assumptions necessary for these reliability estimates to be valid are grossly violated, most commonly, independence. This is particularly problematic in social data, where dependence is both complex and ubiquitous, so much so that it is often of primary interest, as is in analyses of social networks. Another difficulty is other sources of uncertainty which are more difficult to measure and adjust for. It may also be the case that uncertainty from non-random measurement error is substantially more important than random measurement error (i.e., sampling error) in many cases, and instances where non-random measurement error is identified or where multiple measures of the same (or similar) concepts produce substantially different conclusions constitute anecdotal evidence that this is the case [E.g., @LupuJones; @baum2015filtering; @jerven2013poor]. All of this suggests that many measures of uncertainty should be viewed with skepticism and so I have not focused on measuring reliability herein. However, all of the software referenced herein is capable of using estimates of the variability of a models' predictions to produce reliability bounds for the aforementioned types of interpretation tasks which correspond to the quantities that typically accompany the equivalent outputs from conventional methods.

<!-- talk about the extrapolation stuff? or reference it -->

## Approximating and Interpreting Functional Form

When the most complex functional form capable of learned from the data is simple, as in a linear additive model, the parameters of said model are often directly interpretable. Some statistical learning methods have similar properties [See, e.g., @hainmueller2013kernel; @beck2000improving]. However, many such methods cannot be directly interpreted but may be otherwise desirable for a given application: due to, e.g., their ability to deal with large N or many covariates, their predictive performance in this or similar applications, etc.

One of the most common types of interpretation for conventional methods is functional form; i.e., for a linear model what is the sign and magnitude of the coefficient for a covariate, defining the slope of the line representing its relationship with the models predictions. For conventional models where one or more of the covariates have a nonlinear relationship in the model, e.g., a logistic regression, the effect of a covariate on the models' predictions depends on the other covariates. To understand the effect of a covariate then, it is necessary to compute predictions at particular values of the other covariates. An average marginal effect gives the effect of a particular covariate computed at, rather than fixed values for the other covariates, every observed combination of the other covariates. This is a form of Monte-Carlo integration which also works with statistical learning methods, where it is referred to as partial dependence. It allows the recovery of the parital relationship between particular covariates and $\hat{f}$ regardless of how $\hat{f}$ is estimated [@friedman2001greedy].

### Partial Dependence

![This figure shows the partial dependence: the marginalized prediction function, for each component of the model. The black lines, or dots, show the partial dependence estimated from a random forest estimate of the data generating function of Equation \ref{eq:dgp}, while the red line shows the 'truth', the partial dependence of the indicated covariates on the data generating function without any noise. In the top left panel is shown the partial dependence of the discrete covariate $\mathbf{x}$, and in the bottom left its interaction with the continuous covariate $\mathbf{w}$. In the upper and lower right panels the partial dependence of the covariates which have nonlinear effects, $\mathbf{y}$ and $\mathbf{w}$ are shown. While a conventional method could estimate Equation \ref{eq:dgp}, the random forest estimated these relationships without prespecification (albeit in a black-box manner), and with partial dependence we are able to recover them. \label{fig:pd}](pd.png)

If we knew the joint distribution $\mathbb{P}$ of $\mathbf{X}$ we could visualize the dependence between some some subset of $\mathbf{X}$, say $\mathbf{X}_u$, by integrating out the compliment $\mathbf{x}_{-u}$ and computing, e.g., the conditional expectation of $Y | X_u$. However, since we do not know $\mathbb{P}$ and have instead estimated this expectation we can instead use Monte-Carlo integration using the sample data. We would compute

$$\mathbb{E}(\hat{f}(\mathbf{x}) | \mathbf{x}_u) = \int_{{x}_{-u}} \hat{f}(\mathbf{x}_u, \mathbf{x}_{-u}) \mathbb{P}\mathbb(\mathbf{x}_u | \mathbf{x}_{-u}) \mathbb{P}(\mathbf{x}_{-u}) d \mathbf{x}_{-u}.$$ {#eq:ce}

<!-- insert graph from mmpf paper? -->

However the conditional probability of $X_u | X_{-u}$ introduces into the desired description of the relationship between $\mathbf{x}_u$ and $\hat{f}$ dependencies between $\mathbf{x}_u$ and $\mathbf{x}_{-u}$. If this term is dropped, we get the *partial dependence* of $\hat{f}(\mathbf{x})$ on $\mathbf{x}_u$, which can be estimated from the sample data [@friedman2001greedy].[^mmpf]:

[^mmpf]: The effect of the inclusion of this conditional probability on the estimate of the effect of how $\hat{f}$ depends on $\mathbf{X}_u$ can be substantial even when there is no interaction between $\mathbf{X}_u$ and $\mathbf{X}_{-u}$ [@mmpf].

$$\mathbb{E}_{X_{-u}}[f(X)] = \int_{X_{-u}} f(X) d\mathbb{P}(X_{-u}) \approx \frac{1}{N} \sum_{i = 1}^N \hat{f}(\mathbf{x}_u, \mathbf{x}_{-u}^{(i)}) = \bar{f}_{\mathbf{x}_u}(\mathbf{x}_u)$$ {#eq:pd}

This gives the relationship between the prediction made by $\hat{f}(\mathbf{x})$ and $\mathbf{x}_u$ averaged across the observed values of $\mathbf{x}_{-u}$. A suitable grid of points on $\mathbf{x}_u$ must be chosen. This can be done by taking a Monte-Carlo sample from the empirical distribution of $\mathbf{x}_u$ or by selecting values of interest.

How closely $\bar{f}_{\mathbf{x}_u}(\mathbf{x}_u)$ corresponds to $\hat{f}(\mathbf{x}_u)$ depends on the structure of $\hat{f}$. In the case $\hat{f}(\mathbf{x})$ is additive or multiplicative in $\{\mathbf{x}_u, \mathbf{x}_{-u}\}$, that is, $\hat{f}(\mathbf{x}) = \bar{f}_{\mathbf{x}_u}(\mathbf{x}_u) + \bar{f}_{\mathbf{x}_{-u}}(\mathbf{x}_{-u})$ or $\hat{f}(\mathbf{x}) = \bar{f}_{\mathbf{x}_u}(\mathbf{x}_u)\bar{f}_{\mathbf{x}_{-u}}(\mathbf{x}_{-u})$ computing the partial dependence gives an approximation to $\hat{f}(\mathbf{x})$ that is accurate to an additive or multiplicative constant, respectively [@friedman2001greedy; @mmpf]. Discovering when such (additive or multiplicative) factorizable structure is *not* present is a question about interaction, that is, non-separable dependence between $\mathbf{x}_u$ and $\mathbf{x}_{-u}$ in $\hat{f}$.

One way to detecting interaction amongst the covariates using partial dependence is to estimate the variability of [@eq:pd]. This can be done again using Monte-Carlo methods by substituting the sample variance for the mean. Another possibility is to avoid Monte Carlo integration over the distribution of $\mathbf{x}_{-u}$ and to instead visualize the expected value of the outcome at $\mathbf{x}_u$ for each observation conditional on $\mathbf{x}_{-u}$. @goldstein2015peeking propose using this approach. 

$$\mathbb{E}_{\mathbf{x}_{-u}^{(i)}}[\hat{f}(\mathbf{x})] \approx \hat{f}(\mathbf{x}_u, \mathbf{x}_{-u}^{(i)}) = \bar{f}^{(i)}_{\mathbf{x}_u}(\mathbf{x}_u)$$ {#eq:ice}

This is the partial dependence of $\mathbf{x}_u$ on $\hat{f}(\mathbf{x}^{(i)})$, which has some advantages over [@eq:pd]. When there is interaction between $\mathbf{x}_u$ and $\mathbf{x}_{-u}$ in $\hat{f}(\mathbf{x})$ [@eq:ice] will show variation in the estimated individual conditional expectation since the relationship between $\mathbf{x}_u$ and $\hat{f}(\mathbf{x})$ depends on $\mathbf{x}_{-u}$. Hence, regions of interaction can be discovered by observing regions of high variance in [@eq:ice]. Although this can be visualized directly by directly estimating the Monte Carlo variance rather than the mean, visualizing the individual conditional expectation allows for simultaneous visualization of regions of interaction and the the functional form of the relationship between $\hat{f}(\mathbf{x})$ and $\mathbf{x}_u$. This comes at the cost of less interpretability. For each curve in [@eq:pd], [@eq:ice] produces $N$ curves. One approach making these visualizations more interpretable is to center the individual conditional expectaton curves, by, e.g., subtracting the individual conditional expectation of the smallest $\mathbf{x}_u$. Then variation in the curves relative to this baseline may be observed.

It is often the case that rather than the expected value, an "effect," a change in the expected value of the outcome for a change in the covariates, is of direct interest. The partial derivative of each curve in [@eq:ice] or [@eq:pd], the partial derivative of $\hat{f}(\mathbf{x})$ with respect to $\mathbf{x}_u$ can also be estimated using numerical methods. This serves as another way to discover interactions between $\mathbf{x}_u$ and $\mathbf{x}_{-u}$. If there is interaction then the partial derivative will depend on both $\mathbf{x}_u$ and $\mathbf{x}_{-u}$, while if there is no interaction the derivative will not. Hence variation in the derivative of the individual conditional expectation indicates interaction.

All of the above can be quickly and easily estimated for any prediction function $\hat{f}$, regardless of how it was estimated (i.e., both conventional and statistical learning methods) from the sample data using **M**onte **C**arlo **M**arginalization of **P**rediction **F**unctions (`mmpf`) which is a publically available `R` package. The **M**achine **L**earning in **R** (`mlr`) package also makes all of this functionality available, and, additionally, makes available a wide variety of statistical learning methods available for use in a consistent manner, with a wide variety of tools for evaluating and tuning their performance [@mlr; @mmpf]. Lastly, **E**xploratory **D**ata **A**nalysis using **R**andom **F**orests (`edarf`) again provides similar functionality to what is described above along with additional interpretation methods specific to random forests, which are beyond the scope of this paper [@edarf].

## Variable Importance

![Permutation importance estimated from the data generating function in Equation \ref{eq:dgp}. This indicates that the most important component of the model is the interaction of $\mathbf{x}$ and $\mathbf{w}$. Since $\mathbf{x}$ and $\mathbf{w}$ interact in Equation \ref{eq:dgp} and the random forest estimated this interaction, as can be seen in Figure \ref{fig:pd}, permuting either or both covariates will increase prediction error substantially, as they introduce error from the individual components as well as their interaction. In this way permutation importance gives weight to covariates like $\mathbf{w}$ which may only have a conditional effect. Components with less variability, like $\sin(\mathbf{z})$ (which has a sample variance of $0.592$) have less importance than those with more variability (e.g., $\mathbf{y^2}$, which has a sample variance of $1.418$). \label{fig:pi}](pi.png)

The question of which and how important covariates are is a question that arises with predictive and exploratory tasks. In case of the former it is often advantageous to remove covariates which are not useful in predicting the outcome and in the latter case computing the importance of covariates is useful in finding strong predictive relationships for further study. For example these methods were used by @HillJones to rank covariates according to their importance to accurately predicting different measures of human rights abuses. As previously mentioned in the case where the derivative of the model with respect to a covariate is constant the importance of that covariate in that model is summarized by a single number whose magnitude is often of primary interest. With models where this derivative is not constant there is no straightforward way to summarize the importance of a covariate with a single number; fortunately, there are methods which have generalized the notion of importance to models of an arbitrary form. Variable importance methods define importance as a covariate's contribution to decreasing prediction (in particular generalization) error.

Permutation importance is a general method for estimating variable importance which is agnostic to the learning method used. It is the Monte Carlo expectation of the change in prediction error that occurs when $\mathbf{X}_u$ is permuted. If $\mathbf{X}_u$ was not useful to $\hat{f}(\mathbf{x})$ in making predictions then we would expect no change or a decrease in prediction error, whereas if $\mathbf{X}_u$ is important there should be an increase in prediction error when it is permuted. Due to Monte-Carlo error from the permutation, this is repeated $M$ times to give the expectation of the change in the loss $\mathcal{L}$ from permuting $\mathbf{X}_u$.

$$I_{\mathbf{X}_u} = \frac{1}{M} \sum_{j = 1}^M \mathcal{L}\left(\hat{f}(\mathbf{X}_{u \pi^{(j)}}, \mathbf{X}_{-u}), \hat{f}(\mathbf{X})\right)$$

Take for example a linear model of the form $\hat{f}(\mathbf{X}) = \beta_u \mathbf{X}_u + \beta_{-u} \mathbf{X}_{-u}$. Say that $\beta_{-u}$ was $0$ for each column of $\mathbf{X}_{-u}$. Then if we randomly permuted $\mathbf{X}_{-u}$ $M$ times, and, for each one of those times we computed $\hat{f}$ using this randomly shuffled version of $\mathbf{X}_{-u}$, we would find that the prediction error did not change at all, because $\mathbf{X}_{-u}$ does not contribute to the prediction made by $\hat{f}(\mathbf{X})$. Conversely, if we permuted $\mathbf{X}_u$, and $\beta_u$ was non-zero, perhaps relatively large, we would be multiplying a randomized matrix of covariates by large numbers $\beta_u$, and, unsurprisingly, this will make $\hat{f}$ also basically random, and consequently its predictions awful, which would allow us to conclude that $\mathbf{X}_u$ is important in determining $\hat{f}$. Clearly this is unecessary in this simple case where we could simply inspect the parameters $\beta$, but with statistical learning methods this is not usually possible.

Measuring the change in the loss that results from permuting certain covariates as a way of estimating predictive importance is agnostic to the choice of loss function. With categorical (ordered or unordered) outcome variables category-specific or overall (e.g., average agreement across categories) importance can be computed, which can be useful in cases where categories are imbalanced or the costs of making prediction errors differs across the categories, e.g., as may be the case when predicting violence, where an absence of violence is more common than violence but violence is often costly when not anticipated [@berk2009forecasting]. With a continuous outcome the permutation importance can be computed for the mean, i.e., how permuting a covariate affects the conditional expectation function, or any other summary of the distribution of outcomes, such as a quantile. Lastly the permutation importance may not be aggregated at all, giving an observation specific measure of importance. This can, for example, be used to estimate the importance of covariates across the distribution of the outcome (a continuous analogue to category-specific importance).

Permutation importance is certainly not the only method for computing predictive importance with statistical learning methods. There are many method specific measures for Random Forests have been subject to study
[@strobl2007bias; @strobl2008conditional; @altmann2010permutation; @nicodemus2010behaviour; @schwarz2010safari; @janitza2013auc; @gromping2009variable; @louppe2013understanding]. @friedman2001greedy proposed permutation importance as well as other generic methods such as that of @van2006statistical which may prove useful for political scientists but which are beyond the scope of this paper. At this point the theoretical study of this method is limited: @louppe2013understanding investigates the properties of a random forest specific method with a random forest like method which is tractable for his purposes but would not be used in practice. 

<!-- theory here? -->

@van2006statistical
@louppe2013understanding
@friedman2001greedy

Permutation importance as described above is implemented in `edarf`, `mmpf`, and `mlr`.

### Functional ANOVA

Another approach to making models interpretable is the Analysis of Variance applied to functions: the functional ANOVA [@hooker2004discovering; @hooker2012generalized]. The functional ANOVA finds the best possible decomposition of a model in terms of additive components, where the additive components depend on a number of covariates set by the user. 

<!-- Although statistical learning methods generally learn functions which are not directly interpretable, it may be the case that the learned function $\hat{f}(\mathbf{x})$ is representable as a sum of lower dimensional functions which are interpretable with little loss of fidelity. Analysis of Variance (ANOVA) is commonly used to decompose a function which is composed of categorical covariates by treating the function as a sum of discrete effects. The functional ANOVA decomposes a function of covariates of any type as a sum of functions which are lower dimensional than the learned function. --> 

That is, $\hat{f}$ can be represented as a constant plus functions of one variable, functions of two variables, up to a number fixed by the user.

$$\hat{f}(\mathbf{x}) = g_0 + \sum_{i = 1}^p g_{i}(X_i) + \sum_{i \neq j} g_{ij}(X_{ij}) + \ldots$$

Another way to write this is by using the subset operator. Here $u$ is some proper subset of the covariate indices, e.g., $\{1, 2\}$ or $\{2, 3\}$. Each $g_u$ here is the projection of $\hat{f}$ onto the space of functions (the $g$) additive in $u$.[^standard] Each of the components $g$ is constrained to be hierarchically orthogonal, that is, unrelated to components which are proper subset of $u$. $\hat{f}$ can be completely decomposed as

$$\hat{f}(\mathbf{X}) \approx g_u(\mathbf{x}_u) + \sum_{v \subset u} g_v(\mathbf{x}_v) + g_{-u}(\mathbf{x}_{-u}) + \sum_{i \in u \subset v^{\prime} \subseteq -i} g_{v^{\prime}}(\mathbf{x}_{v^\prime})$$

The components of this decomposition are the solution to the optimization problem:

$$\arg\min_{g} \left(g_0 + g_u(\mathbf{x}_u) + \sum_{v \subset u} g_v(\mathbf{x}_v) + g_{-u}(\mathbf{x}_{-u}) + \sum_{i \in u \subset v^{\prime} \subseteq -i} g_{v^{\prime}}(\mathbf{x}_{v^\prime}) - \hat{f}(\mathbf{x})\right)^2$$

which can be estimated using a pointwise estimation scheme described in @hooker2012generalized and implemented in the `fanova` package and also available in `mlr`. As described in @hooker2004discovering this can be used to, not only estimate the aformentioned components, but also to measure the loss that occurs when excluding them from the decomposition, giving another method of variable importance.

[^standard]: When $u$ is a single covariate, $g_u(\mathbf{x}_u) = \bar{f}_u(\mathbf{x}_u)$. So each $g_u$ can be estimated by [@eq:pd].

The functional ANOVA is closely related to other variance decomposition methods like ... (cite french papers). 

## Extrapolation

 - existing approaches (convex hull)
 - multivariate density estimation
 
A key issue with the abovementioned methods is extrapolation (See, e.g., @king2006dangers for a discussion of the issue in political science). Regions of [@eq:pd] and [@eq:ice], i.e., combinations of $\mathbf{x}_u$ and $\mathbf{x}_{-u}$ or $\mathbf{x}_{-u}^{(i)}$ may have low probability under the joint distribution. Any estimate of the behavior of $\hat{f}(\mathbf{x})$ in this region will have high variance, i.e., it may be interpreted less reliably. It is therefore desireable to detect such regions visually, and, further, to avoid constructing approximations to $\hat{f}(\mathbf{x})$ that are strongly influenced by such regions.

Regions of extrapolation can be detected visually with [@eq:ice] by, for each curve, adding a point which indicates the observed value of $\mathbf{x}_u$. The density of the points at a given point in $\mathbf{x}_u$ gives an estimate of the marginal density of $\mathbf{x}_u$ at said point. This can be particularly useful when $\mathbf{x}_u$ is multidimensional.

## Summary and Future Work

## References
